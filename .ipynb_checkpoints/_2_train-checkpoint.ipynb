{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f90381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.13.0', '1.3.0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "\n",
    "tf.__version__, sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e11987",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6ad1a4",
   "metadata": {},
   "source": [
    "# 1. Setup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f935d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./_1_setup_dataset.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6040d4a",
   "metadata": {},
   "source": [
    "# 2. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9429acd",
   "metadata": {},
   "source": [
    "### 2.1 modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3da498a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 148, 148, 32)      0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 148, 148, 32)      0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 74, 74, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 72, 72, 64)        0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 72, 72, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 36, 36, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 34, 34, 128)       0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 34, 34, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 17, 17, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 15, 15, 128)       147584    \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 15, 15, 128)       0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 15, 15, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 7, 7, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               3211776   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3454147 (13.18 MB)\n",
      "Trainable params: 3454147 (13.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "def create_model(activation=\"relu\", dropout=0.1, input_shape=(150,150,3), nb_classes=3):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(32, (3,3), input_shape=input_shape))\n",
    "    model.add(layers.Activation(activation))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3,3)))\n",
    "    model.add(layers.Activation(activation))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3,3)))\n",
    "    model.add(layers.Activation(activation))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3,3)))\n",
    "    model.add(layers.Activation(activation))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(512, activation=activation))\n",
    "    model.add(layers.Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "    \n",
    "model = create_model(activation=\"relu\", dropout=0.3, input_shape=(150,150,3), nb_classes=3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caca6f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.sequential.Sequential at 0x1c7c022c890>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False),\n",
    "              metrics=['accuracy'])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c77b2eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define generator\n",
    "\n",
    "# train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# batch_size = 10\n",
    "\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#     train_dir,\n",
    "#     target_size=(150, 150),  # Resize all images to 150x150.\n",
    "#     batch_size=batch_size,\n",
    "#     class_mode='categorical',\n",
    "# )\n",
    "\n",
    "# validation_generator = test_datagen.flow_from_directory(\n",
    "#     validation_dir,\n",
    "#     target_size=(150, 150),\n",
    "#     batch_size=batch_size,\n",
    "#     class_mode='categorical',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3f3711c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m\n\u001b[0;32m     12\u001b[0m test_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m     14\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     16\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m train_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mtrain_dir\u001b[49m,\n\u001b[0;32m     18\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m150\u001b[39m, \u001b[38;5;241m150\u001b[39m),  \u001b[38;5;66;03m# Resize all images to 150x150.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     20\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m validation_generator \u001b[38;5;241m=\u001b[39m test_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[0;32m     24\u001b[0m     validation_dir,\n\u001b[0;32m     25\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m150\u001b[39m, \u001b[38;5;241m150\u001b[39m),\n\u001b[0;32m     26\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     27\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     28\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# define generator with data augumentation\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),  # Resize all images to 150x150.\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4296b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set early-stopping\n",
    "\n",
    "best_model_h5 = \"best_model_{}.h5\".format(tf.__version__)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", min_delta=1e-5, patience=5, mode=\"auto\", verbose=0),\n",
    "             ModelCheckpoint(filepath=Path(MODEL_DIR, best_model_h5), monitor=\"val_loss\", save_best_only=True, save_weights_only=False),\n",
    "             ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=10),  # divide learning_rate by 10 when the callback is invoked.\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4099c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "epochs = 30\n",
    "# epochs = 2\n",
    "\n",
    "begin_time = datetime.now()\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"\\nElapsed Time: {}\\t{}\".format(datetime.now() - begin_time, datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd49b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw convergence history\n",
    "\n",
    "def draw_convergence_history(history, save_file=None):\n",
    "    \"\"\"draw convergence history\"\"\"\n",
    "    \n",
    "    score = list(history.keys())[1]\n",
    "    val_score = list(history.keys())[3]\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(10,4), facecolor=\"w\")\n",
    "    \n",
    "    # obtain the history of training loss and validation loss\n",
    "    training_loss = history[\"loss\"]\n",
    "    validation_loss = history[\"val_loss\"]\n",
    "\n",
    "    # prepare epoch list\n",
    "    epoch_count = range(1, len(training_loss) + 1)\n",
    "    \n",
    "    # plot \"loss convergence\" for training data and validation data\n",
    "    ax[0].plot(epoch_count, training_loss, \"rx--\")  # r-- ro ro- ro--\n",
    "    ax[0].plot(epoch_count, validation_loss, \"bo-\")\n",
    "    \n",
    "    ax[0].set_title(\"convergence history of loss\")\n",
    "    ax[0].legend([\"Training Loss\", \"Validation Loss\"], loc=\"upper right\")\n",
    "    ax[0].set_xlabel(\"Epoch\")\n",
    "    ax[0].set_ylabel(\"Loss\")\n",
    "    ax[0].grid()\n",
    "\n",
    "    # plot \"accuracy convergence\" for training data and validation data\n",
    "    training_accuracy = history[score]\n",
    "    validation_accuracy = history[val_score]\n",
    "    \n",
    "    ax[1].plot(epoch_count, training_accuracy, \"rx--\")  # r-- ro ro- ro--\n",
    "    ax[1].plot(epoch_count, validation_accuracy, \"bo-\")\n",
    "    \n",
    "    ax[1].set_title(\"convergence history of score\")\n",
    "    ax[1].legend([\"Training Score\", \"Validation Score\"], loc=\"lower right\")\n",
    "    ax[1].set_xlabel(\"Epoch\")\n",
    "    ax[1].set_ylabel(\"Score\")\n",
    "    ax[1].grid()\n",
    "\n",
    "    if save_file:\n",
    "        plt.savefig(save_file)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "draw_convergence_history(history.history, save_file=Path(OUTPUT_DIR, \"convergence_history.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd869f99",
   "metadata": {},
   "source": [
    "### 2.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de98b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define generator\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1d03d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss and accuracy\n",
    "\n",
    "loss, accuracy = model.evaluate(test_generator, verbose=1)\n",
    "print(f\"{loss= :.4f}\\t{accuracy= :.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d163de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict testset\n",
    "\n",
    "test_pred_dict = {}\n",
    "\n",
    "for test_dir in test_dirs:\n",
    "    pred = []\n",
    "        \n",
    "    for path in test_dir.iterdir():\n",
    "#         print(path)\n",
    "        img = image.load_img(path, target_size=(150, 150))\n",
    "        img_tensor = image.img_to_array(img)\n",
    "        img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "        img_tensor /= 255.\n",
    "\n",
    "        p = model.predict(img_tensor)[0]\n",
    "        pred.append(np.argmax(p))\n",
    "    \n",
    "    test_pred_dict[test_dir.name] = pred\n",
    "\n",
    "\n",
    "test_pred = [*test_pred_dict[\"salad\"], *test_pred_dict[\"sushi\"], *test_pred_dict[\"tofu\"]]\n",
    "print(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae19daa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ture class list \n",
    "\n",
    "num_of_salad = len(os.listdir(test_salad_dir))\n",
    "num_of_sushi = len(os.listdir(test_sushi_dir))\n",
    "num_of_tofu = len(os.listdir(test_tofu_dir))\n",
    "\n",
    "test_true = [*[0]*num_of_salad, *[1]*num_of_sushi, *[2]*num_of_tofu]\n",
    "print(test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41276345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "\n",
    "report = metrics.classification_report(test_true, test_pred)\n",
    "\n",
    "print(f\"classification report on test dataset\\n{report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b139408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(test_true, test_pred)\n",
    "\n",
    "sns.heatmap(confusion_matrix, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "    \n",
    "plt.savefig(Path(OUTPUT_DIR, \"confusionmatrix.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1224b1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
